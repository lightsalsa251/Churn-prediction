{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing, EDA and FE all are completed now  \n",
    "Next, we will train our model with lightgbm  \n",
    "We are not normalizing our data, i.e not changing the distribution and tree methods work very well with these un-normalized values  \n",
    "Plus LGBM is very fast and is a serial kaggle competition winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle('np/df_test.pickle')\n",
    "X_train = pd.read_pickle('np/df.pickle')\n",
    "\n",
    "y_train = X_train['exited'].values\n",
    "y_test = X_test['exited'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('exited', axis = 1)\n",
    "X_test = X_test.drop('exited', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"binary_logloss\",\n",
    "        \"num_leaves\" : 20,\n",
    "        \"learning_rate\" : 0.007,\n",
    "        \"bagging_fraction\" : 0.6,\n",
    "        \"feature_fraction\" : 0.6,\n",
    "        \"bagging_freq\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgtrain, lgval], early_stopping_rounds=100, \n",
    "                      verbose_eval=200, evals_result=evals_result)\n",
    "    \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.376959\tvalid_1's binary_logloss: 0.375503\n",
      "[400]\ttraining's binary_logloss: 0.283317\tvalid_1's binary_logloss: 0.281733\n",
      "[600]\ttraining's binary_logloss: 0.243527\tvalid_1's binary_logloss: 0.243311\n",
      "[800]\ttraining's binary_logloss: 0.222816\tvalid_1's binary_logloss: 0.22637\n",
      "[1000]\ttraining's binary_logloss: 0.209904\tvalid_1's binary_logloss: 0.217825\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.209904\tvalid_1's binary_logloss: 0.217825\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.375016\tvalid_1's binary_logloss: 0.383526\n",
      "[400]\ttraining's binary_logloss: 0.280354\tvalid_1's binary_logloss: 0.29425\n",
      "[600]\ttraining's binary_logloss: 0.239134\tvalid_1's binary_logloss: 0.258185\n",
      "[800]\ttraining's binary_logloss: 0.217876\tvalid_1's binary_logloss: 0.242845\n",
      "[1000]\ttraining's binary_logloss: 0.204616\tvalid_1's binary_logloss: 0.235253\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.204616\tvalid_1's binary_logloss: 0.235253\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.3759\tvalid_1's binary_logloss: 0.38171\n",
      "[400]\ttraining's binary_logloss: 0.28211\tvalid_1's binary_logloss: 0.292275\n",
      "[600]\ttraining's binary_logloss: 0.241512\tvalid_1's binary_logloss: 0.255719\n",
      "[800]\ttraining's binary_logloss: 0.220232\tvalid_1's binary_logloss: 0.239242\n",
      "[1000]\ttraining's binary_logloss: 0.207023\tvalid_1's binary_logloss: 0.231703\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.207023\tvalid_1's binary_logloss: 0.231703\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.37787\tvalid_1's binary_logloss: 0.386203\n",
      "[400]\ttraining's binary_logloss: 0.28212\tvalid_1's binary_logloss: 0.294455\n",
      "[600]\ttraining's binary_logloss: 0.239696\tvalid_1's binary_logloss: 0.257627\n",
      "[800]\ttraining's binary_logloss: 0.217934\tvalid_1's binary_logloss: 0.242383\n",
      "[1000]\ttraining's binary_logloss: 0.204424\tvalid_1's binary_logloss: 0.235476\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.204424\tvalid_1's binary_logloss: 0.235476\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.375368\tvalid_1's binary_logloss: 0.382\n",
      "[400]\ttraining's binary_logloss: 0.281672\tvalid_1's binary_logloss: 0.291597\n",
      "[600]\ttraining's binary_logloss: 0.2414\tvalid_1's binary_logloss: 0.254113\n",
      "[800]\ttraining's binary_logloss: 0.220671\tvalid_1's binary_logloss: 0.237411\n",
      "[1000]\ttraining's binary_logloss: 0.207788\tvalid_1's binary_logloss: 0.228681\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.207788\tvalid_1's binary_logloss: 0.228681\n",
      "Seed 42 completed....\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.373127\tvalid_1's binary_logloss: 0.389802\n",
      "[400]\ttraining's binary_logloss: 0.278076\tvalid_1's binary_logloss: 0.301961\n",
      "[600]\ttraining's binary_logloss: 0.237008\tvalid_1's binary_logloss: 0.266658\n",
      "[800]\ttraining's binary_logloss: 0.215806\tvalid_1's binary_logloss: 0.251163\n",
      "[1000]\ttraining's binary_logloss: 0.202684\tvalid_1's binary_logloss: 0.244226\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.202684\tvalid_1's binary_logloss: 0.244226\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.375422\tvalid_1's binary_logloss: 0.387347\n",
      "[400]\ttraining's binary_logloss: 0.281078\tvalid_1's binary_logloss: 0.29568\n",
      "[600]\ttraining's binary_logloss: 0.240958\tvalid_1's binary_logloss: 0.256606\n",
      "[800]\ttraining's binary_logloss: 0.220273\tvalid_1's binary_logloss: 0.239476\n",
      "[1000]\ttraining's binary_logloss: 0.206858\tvalid_1's binary_logloss: 0.230932\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.206858\tvalid_1's binary_logloss: 0.230932\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.37762\tvalid_1's binary_logloss: 0.376858\n",
      "[400]\ttraining's binary_logloss: 0.28277\tvalid_1's binary_logloss: 0.28802\n",
      "[600]\ttraining's binary_logloss: 0.241579\tvalid_1's binary_logloss: 0.253863\n",
      "[800]\ttraining's binary_logloss: 0.2204\tvalid_1's binary_logloss: 0.239051\n",
      "[1000]\ttraining's binary_logloss: 0.206907\tvalid_1's binary_logloss: 0.230993\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.206907\tvalid_1's binary_logloss: 0.230993\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.378355\tvalid_1's binary_logloss: 0.374236\n",
      "[400]\ttraining's binary_logloss: 0.284623\tvalid_1's binary_logloss: 0.282073\n",
      "[600]\ttraining's binary_logloss: 0.243363\tvalid_1's binary_logloss: 0.244434\n",
      "[800]\ttraining's binary_logloss: 0.222354\tvalid_1's binary_logloss: 0.227618\n",
      "[1000]\ttraining's binary_logloss: 0.209404\tvalid_1's binary_logloss: 0.219244\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.209404\tvalid_1's binary_logloss: 0.219244\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's binary_logloss: 0.37605\tvalid_1's binary_logloss: 0.377581\n",
      "[400]\ttraining's binary_logloss: 0.283198\tvalid_1's binary_logloss: 0.285865\n",
      "[600]\ttraining's binary_logloss: 0.242906\tvalid_1's binary_logloss: 0.248016\n",
      "[800]\ttraining's binary_logloss: 0.221862\tvalid_1's binary_logloss: 0.231946\n",
      "[1000]\ttraining's binary_logloss: 0.208545\tvalid_1's binary_logloss: 0.224527\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.208545\tvalid_1's binary_logloss: 0.224527\n",
      "Seed 2018 completed....\n",
      "LightGBM Training Completed...\n"
     ]
    }
   ],
   "source": [
    "# Training LGB\n",
    "seeds = [42, 2018]\n",
    "pred_test_full_seed = 0\n",
    "for seed in seeds:\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    pred_test_full = 0\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = X_train.loc[dev_index,:], X_train.loc[val_index,:]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\n",
    "        pred_test_full += pred_test\n",
    "    pred_test_full /= 5.\n",
    "    pred_test_full = np.expm1(pred_test_full)\n",
    "    pred_test_full_seed += pred_test_full\n",
    "    print(\"Seed {} completed....\".format(seed))\n",
    "pred_test_full_seed /= np.float(len(seeds))\n",
    "\n",
    "print(\"LightGBM Training Completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Importance...\n",
      "         feature  split       gain\n",
      "12        active   1282  22.740450\n",
      "8            age   2679  18.445806\n",
      "4             c3   1385   8.746731\n",
      "9        balance   2982   6.974028\n",
      "14           p_2    652   6.680613\n",
      "0        spanish   1213   6.216665\n",
      "11      has_card   1269   5.749348\n",
      "10      products   1183   5.136999\n",
      "1         german    987   4.648748\n",
      "7         gender    711   4.574573\n",
      "5             c4   1055   4.018991\n",
      "6   credit_score   2035   2.005409\n",
      "2             c1    674   1.644750\n",
      "13           p_1    281   1.217403\n",
      "3             c2    482   0.743930\n",
      "15           p_3    130   0.455555\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "print(\"Features Importance...\")\n",
    "gain = model.feature_importance('gain')\n",
    "featureimp = pd.DataFrame({'feature':model.feature_name(), \n",
    "                   'split':model.feature_importance('split'), \n",
    "                   'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "print(featureimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04974974, 0.04771815, 0.17825329, ..., 0.8091257 , 0.15507791,\n",
       "       0.40616478])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred < 0.5] = 0\n",
    "pred[pred >= 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test has less than 20% exited cases. So, kappa metric should be used instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52827210808005"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kappa can be in the range -1 and +1  \n",
    "Anything above +0.5 is a strong agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
